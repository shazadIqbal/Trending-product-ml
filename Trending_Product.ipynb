{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d26e6b-108c-410d-a50b-ae6812c4ce9e",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "\n",
    "Installs necessary libraries like praw (Reddit API), google-api-python-client (YouTube API), pytrends (Google Trends API), python-dotenv (for environment variables).\n",
    "Imports required Python libraries for data processing (pandas, re, nltk, etc.).\n",
    "Installs and imports machine learning libraries (transformers, torch, datasets, scikit-learn).\n",
    "Uses DistilBERT from Hugging Face for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1fa0c3-375b-4889-83a4-a5bf977256c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: praw in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (2.160.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-python-client) (2.38.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-python-client) (2.24.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytrends in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (4.9.2)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from pytrends) (2.32.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytrends) (2.1.4)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from pytrends) (4.9.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->pytrends) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.0->pytrends) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (4.48.3)\n",
      "Requirement already satisfied: torch in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (2.6.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (3.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (3.8.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (4.48.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shahzad iqbal\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahzad Iqbal\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!pip install praw\n",
    "!pip install google-api-python-client\n",
    "!pip install pytrends\n",
    "!pip install python-dotenv\n",
    "import praw\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from pytrends.request import TrendReq\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "!pip install pandas nltk transformers torch datasets scikit-learn seaborn\n",
    "!pip install transformers[torch] accelerate -U\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "import transformers\n",
    "import torch\n",
    "import accelerate\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b6dff3-f5e2-4c3f-a3c6-3cdf553a1a4c",
   "metadata": {},
   "source": [
    "##### Checks and prints the versions of transformers, torch, and accelerate to ensure compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e22ad78-6ecc-4c64-95de-c287ef2014e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.48.3\n",
      "Torch Version: 2.6.0+cpu\n",
      "Accelerate Version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformers Version:\", transformers.__version__)\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"Accelerate Version:\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf3982-e683-40b3-9641-f9ff7faee171",
   "metadata": {},
   "source": [
    "##### Loads API credentials from the .env file.\n",
    "##### Prints environment variables to verify that the credentials are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49638ff-6be1-4d6d-ad22-21c87f811b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "print(os.getenv(\"REDDIT_CLIENT_ID\"))\n",
    "print(os.getenv(\"REDDIT_CLIENT_ID\"))\n",
    "print(os.getenv(\"REDDIT_CLIENT_SECRET\"))\n",
    "print(os.getenv(\"REDDIT_USER_AGENT\"))\n",
    "print(os.getenv(\"YOUTUBE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df8e60-2216-49c0-8390-1c6f2c3d934b",
   "metadata": {},
   "source": [
    "# Reddit\n",
    "##### Initializes the Reddit API client in read-only mode.\n",
    "##### Defines get_reddit_posts() function to fetch top posts from specific subreddits related to smartwatches.\n",
    "##### Stores retrieved data in a Pandas DataFrame with columns title, score, and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04c6fa1-c600-433a-a7e2-ce64e39ffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "                                               title  score  \\\n",
      "0                              Suggest me smartwatch      2   \n",
      "1  What is the best Smartwatch for iPhone widely ...      2   \n",
      "2                        Smartwatch for working out?      2   \n",
      "3            Is there a market for old Smartwatches?      4   \n",
      "4             Best smartwatch for $150 - $200 budget      2   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.reddit.com/r/smartwatch/comments/1...  \n",
      "1  https://www.reddit.com/r/smartwatch/comments/1...  \n",
      "2  https://www.reddit.com/r/smartwatch/comments/1...  \n",
      "3             https://www.reddit.com/gallery/1ijcxxm  \n",
      "4  https://www.reddit.com/r/smartwatch/comments/1...  \n"
     ]
    }
   ],
   "source": [
    "# Reddit API Credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"REDDIT_USER_AGENT\")\n",
    ")\n",
    "\n",
    "print(reddit.read_only) \n",
    "\n",
    "def get_reddit_posts(subreddits, query=\"smartwatch\", limit=1500):\n",
    "    \"\"\"Fetch top posts from multiple subreddits\"\"\"\n",
    "    posts = []\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        for post in reddit.subreddit(subreddit).search(query, sort=\"hot\", limit=limit):\n",
    "            posts.append([post.title, post.score, post.url])\n",
    "    \n",
    "    df = pd.DataFrame(posts, columns=[\"title\", \"score\", \"url\"])\n",
    "    return df\n",
    "\n",
    "# Fetch Reddit smartwatch discussions\n",
    "subreddits = [\"smartwatch\", \"wearables\", \"watches\"]\n",
    "reddit_data = get_reddit_posts(subreddits)\n",
    "print(reddit_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c2e09-96a3-4181-8b43-e8b68ae42cf7",
   "metadata": {},
   "source": [
    "# Fetch YouTube Trending Videos\n",
    "\n",
    "##### Uses YouTube Data API to fetch trending videos related to smartwatches.\n",
    "##### Stores video_id, title, and channel name in a Pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f2d779-63de-4ce0-8ad3-bf957724e395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      video_id                                              title  \\\n",
      "0  7ieKOKwUYoo  Smartwatches von Apple, Samsung, Huawei &amp; ...   \n",
      "1  oFZ2nSozoTs  😍Sim + WiFi 4G LTE Android Watch Tk4 Ultra Fir...   \n",
      "2  BxoSN7FMe-4  Die Besten Smartwatches für Sport und Fitness!...   \n",
      "3  F2DDlM7LIb0  How to add apple logo in smart watch series 7 ...   \n",
      "4  DUFBsdGmZPY  Apple Watch Ultra waterproof test #shorts #app...   \n",
      "\n",
      "             channel  \n",
      "0     ARD Marktcheck  \n",
      "1  Perfect Gadget BD  \n",
      "2      Sport Technik  \n",
      "3    Trend Yourstyle  \n",
      "4             iWatch  \n"
     ]
    }
   ],
   "source": [
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def get_youtube_trending(query=\"smartwatch\", max_results=1000):\n",
    "    \"\"\"Fetch trending YouTube videos related to smartwatches\"\"\"\n",
    "    request = youtube.search().list(\n",
    "        q=query, part=\"snippet\", maxResults=max_results, type=\"video\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    videos = []\n",
    "    for item in response[\"items\"]:\n",
    "        video_id = item[\"id\"][\"videoId\"]\n",
    "        title = item[\"snippet\"][\"title\"]\n",
    "        channel = item[\"snippet\"][\"channelTitle\"]\n",
    "        videos.append([video_id, title, channel])\n",
    "\n",
    "    return pd.DataFrame(videos, columns=[\"video_id\", \"title\", \"channel\"])\n",
    "\n",
    "# Fetch YouTube trending smartwatch videos\n",
    "youtube_data = get_youtube_trending()\n",
    "print(youtube_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912f124-baf7-406f-9e4e-e28fda5d6033",
   "metadata": {},
   "source": [
    "# Clean the Data\n",
    "\n",
    "##### Downloads NLTK stopwords.\n",
    "##### Defines clean_text() function to remove URLs, special characters, and stopwords.\n",
    "##### Cleans title text from Reddit and YouTube datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e824fb-4ecb-4ac6-8d46-a18c7f910439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shahzad\n",
      "[nltk_data]     Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove URLs, special characters, and stopwords from text\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "reddit_data[\"title\"] = reddit_data[\"title\"].apply(clean_text)\n",
    "youtube_data[\"title\"] = youtube_data[\"title\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f123b-6f51-4bb0-ae9f-8b76521f85db",
   "metadata": {},
   "source": [
    "# Merging csv\n",
    "\n",
    "##### Merges Reddit and YouTube data.\n",
    "##### Saves the final dataset as a CSV file in the user’s Downloads folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cf0506-d15a-400b-83af-521dd78ff025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved to: C:\\Users\\Shahzad Iqbal\\Downloads\\trending_smartwatch_data.csv\n",
      "All data saved to trending_smartwatch_data.csv!\n"
     ]
    }
   ],
   "source": [
    "# Merge data from all sources\n",
    "final_data = pd.concat([\n",
    "    reddit_data.assign(source=\"Reddit\"),\n",
    "    youtube_data.assign(source=\"YouTube\")\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "username = os.getlogin()\n",
    "\n",
    "# Define the Downloads folder path\n",
    "downloads_folder = f\"C:\\\\Users\\\\{username}\\\\Downloads\"\n",
    "\n",
    "# Define full file path\n",
    "file_path = os.path.join(downloads_folder, \"trending_smartwatch_data.csv\")\n",
    "\n",
    "# Save CSV file to Downloads folder\n",
    "final_data.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"All data saved to: {file_path}\")\n",
    "print(\"All data saved to trending_smartwatch_data.csv!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69b969-4dae-40c6-87dd-3174798cbc1b",
   "metadata": {},
   "source": [
    "# Preprocess & Assign Sentiment Labels\n",
    "##### Remove URLs, special characters, and stopwords.\n",
    "##### Assign sentiment labels manually based on score:\n",
    "##### Positive (score ≥ 3).\n",
    "##### Neutral (score = 2).\n",
    "##### Negative (score ≤ 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda0fd2e-362b-430e-bac3-b79584257660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Shahzad\n",
      "[nltk_data]     Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suggest smartwatch</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.reddit.com/r/smartwatch/comments/1...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>suggest smartwatch</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best smartwatch iphone widely loved currently</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.reddit.com/r/smartwatch/comments/1...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>best smartwatch iphone widely loved currently</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smartwatch working</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.reddit.com/r/smartwatch/comments/1...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>smartwatch working</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>market old smartwatches</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.reddit.com/gallery/1ijcxxm</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>market old smartwatches</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>best smartwatch budget</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://www.reddit.com/r/smartwatch/comments/1...</td>\n",
       "      <td>Reddit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>best smartwatch budget</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title  score  \\\n",
       "0                             suggest smartwatch    2.0   \n",
       "1  best smartwatch iphone widely loved currently    2.0   \n",
       "2                             smartwatch working    2.0   \n",
       "3                        market old smartwatches    4.0   \n",
       "4                         best smartwatch budget    2.0   \n",
       "\n",
       "                                                 url  source video_id channel  \\\n",
       "0  https://www.reddit.com/r/smartwatch/comments/1...  Reddit      NaN     NaN   \n",
       "1  https://www.reddit.com/r/smartwatch/comments/1...  Reddit      NaN     NaN   \n",
       "2  https://www.reddit.com/r/smartwatch/comments/1...  Reddit      NaN     NaN   \n",
       "3             https://www.reddit.com/gallery/1ijcxxm  Reddit      NaN     NaN   \n",
       "4  https://www.reddit.com/r/smartwatch/comments/1...  Reddit      NaN     NaN   \n",
       "\n",
       "                                    cleaned_text sentiment  \n",
       "0                             suggest smartwatch   Neutral  \n",
       "1  best smartwatch iphone widely loved currently   Neutral  \n",
       "2                             smartwatch working   Neutral  \n",
       "3                        market old smartwatches  Positive  \n",
       "4                         best smartwatch budget   Neutral  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes URLs, special characters, and stopwords.\"\"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z ]\", \"\", text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = \" \".join(word for word in text.split() if word not in stop_words)  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df = final_data\n",
    "# Apply text cleaning\n",
    "df[\"cleaned_text\"] = df[\"title\"].apply(lambda x: clean_text(str(x)))\n",
    "\n",
    "# Assign sentiment labels based on score\n",
    "df[\"sentiment\"] = np.where(df[\"score\"] >= 3, \"Positive\",\n",
    "                           np.where(df[\"score\"] == 2, \"Neutral\", \"Negative\"))\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna(subset=[\"cleaned_text\", \"sentiment\"])\n",
    "\n",
    "# Show cleaned dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfcd8b0-b11a-4bd1-83b8-9223571ecac0",
   "metadata": {},
   "source": [
    "# Encode Sentiment Labels & Split Data\n",
    "\n",
    "##### Encodes sentiment labels into numerical values.\n",
    "##### Splits data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05cb056c-e2e6-4b3a-a984-c0ab93ea1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 511, Testing size: 128\n"
     ]
    }
   ],
   "source": [
    "# Encode sentiment labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"sentiment\"])  # 0=Negative, 1=Neutral, 2=Positive\n",
    "\n",
    "# Train-Test Split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df[\"cleaned_text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training size: {len(train_texts)}, Testing size: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0e9d6-d7fb-4216-8d26-10f343523abe",
   "metadata": {},
   "source": [
    "# Load Pre-Trained DistilBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5641736b-9450-4902-be7d-01d026f7da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize Data\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd3cf8-1868-4562-8b0c-0581caf44ef2",
   "metadata": {},
   "source": [
    "# Convert Data into PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44509a58-b42f-4e0c-81f5-053234e3ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Tokenized Sentiment Data\"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "train_dataset = SentimentDataset(train_encodings, train_labels.tolist())\n",
    "test_dataset = SentimentDataset(test_encodings, test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab56aa4-c339-4cf7-94c3-59d443f345bf",
   "metadata": {},
   "source": [
    "# Train a Multimodal LLM (DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dad645-1446-4e6f-8ad2-bccbfb0107e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Shahzad Iqbal\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 08:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.818787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.741625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.802121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=192, training_loss=0.6638236840566, metrics={'train_runtime': 489.7124, 'train_samples_per_second': 3.13, 'train_steps_per_second': 0.392, 'total_flos': 17848489187970.0, 'train_loss': 0.6638236840566, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Pretrained Model for Sentiment Classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Trainer Object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296065f-79e6-4df1-a443-c9349f00a8d2",
   "metadata": {},
   "source": [
    "# Evaluate & Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f2ff51-ea9d-4118-b689-136433eba3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Save Model\n",
    "model.save_pretrained(\"sentiment_model\")\n",
    "tokenizer.save_pretrained(\"sentiment_model\")\n",
    "\n",
    "print(\"Model trained and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd80237-165c-45e6-a07e-82f4638c7dae",
   "metadata": {},
   "source": [
    "# Load the Trained Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76185e42-761c-43c8-87b2-2c22997fbecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"sentiment_model\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"sentiment_model\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486449f-7ba3-42cd-85ec-5c0cf527b5e5",
   "metadata": {},
   "source": [
    "# Define a Function for Sentiment Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8474c870-e6d3-484d-9b8f-064a0c607825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \"\"\"Predict sentiment of a given text using the trained model.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    sentiment_labels = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    return sentiment_labels[predicted_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef8c15-22df-451b-bbaf-e0caca79cb75",
   "metadata": {},
   "source": [
    "# Test with Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "382b8310-7b43-4dfc-ade5-7cf2992fdf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This smartwatch is amazing! The battery lasts forever. → Sentiment: Neutral\n",
      "Text: The watch is okay, but I expected better features. → Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"This smartwatch is amazing! The battery lasts forever.\",\n",
    "    \"The watch is okay, but I expected better features.\"\n",
    "]\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"Text: {text} → Sentiment: {predict_sentiment(text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7707c6e-977b-48ee-9bba-6cdcc0c68abe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
